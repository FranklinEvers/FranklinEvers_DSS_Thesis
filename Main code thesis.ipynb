{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6eeaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Prepping dataset #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fa5b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries prepping data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6a4424",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing data\n",
    "data = pd.read_csv(\"C:/Users/Frank/Downloads/Thesis/Datasets/Reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9132bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing neutral variable\n",
    "value_to_remove = 3\n",
    "data = data[data['Score'] != value_to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1509ef6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding Sentiment variable\n",
    "def categorize_score(score):\n",
    "    if score in [1, 2]:\n",
    "        return 'negative'\n",
    "    elif score in [4, 5]:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'Missing' \n",
    "\n",
    "data['Sentiment'] = data['Score'].apply(categorize_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3c8484",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a more balanced dataset\n",
    "positive_class = data[data['Sentiment'] == 'positive']\n",
    "negative_class = data[data['Sentiment'] == 'negative']\n",
    "\n",
    "undersampled_positive_class = resample(positive_class, \n",
    "                                      replace=False, \n",
    "                                      n_samples=len(negative_class), \n",
    "                                      random_state=50)\n",
    "\n",
    "undersampled_data = pd.concat([undersampled_positive_class, negative_class])\n",
    "Balanced_data = undersampled_data.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25a4009",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking Balanced dataset numbers\n",
    "class_counts = Balanced_data['Sentiment'].value_counts()\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810c39e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing datasets\n",
    "plt.subplot(1, 2, 1)\n",
    "data['Sentiment'].value_counts().plot(kind='bar', color=['blue', 'orange'])\n",
    "plt.title('Original Class Distribution (a)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "Balanced_data['Sentiment'].value_counts().plot(kind='bar', color=['blue', 'orange'])\n",
    "plt.title('Balanced Class Distribution (b)')\n",
    "\n",
    "plt.subplots_adjust(wspace=1)\n",
    "plt.show()\n",
    "print(Balanced_data[\"Sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a774f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Cleaning text #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6e0c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning & Lowercasing text\n",
    "#Lowercasing\n",
    "data[\"Text\"] = data[\"Text\"].str.lower()\n",
    "\n",
    "#Remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    punctuation_pattern = r'[^\\w\\s]'   \n",
    "    return re.sub(punctuation_pattern, \"\", text)\n",
    "data[\"Text\"] = data[\"Text\"].apply(remove_punctuation)\n",
    "\n",
    "#Remove URL\n",
    "def remove_urls(text):\n",
    "    url_pattern = r'http[s]?://\\S+|www\\.\\S+'\n",
    "    return re.sub(url_pattern, '', text)\n",
    "data['Text'] = data['Text'].apply(remove_urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ddd211",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing stopwords\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceb2c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stopwords.words('english')]\n",
    "    return ' '.join(filtered_words)\n",
    "data['Text'] = data['Text'].apply(remove_stopwords)\n",
    "\n",
    "#Remove extra spaces\n",
    "def remove_extra_spaces(text):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "data['Text'] = data['Text'].apply(remove_extra_spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e3a9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Splitting data #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35811ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d3df41",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Balanced_data[\"Text\"]\n",
    "y = Balanced_data[\"Sentiment\"]\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=50)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c60bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# KNN - BOW - Hyperparameter tuning #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce8a129",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import sklearn\n",
    "import notebook\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665a543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create pipeline BOW\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', KNeighborsClassifier()) \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805e9992",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter grid n-gram optimization\n",
    "param_grid = {\n",
    "    'vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)],\n",
    "    'classifier__n_neighbors': [9, 11, 13] \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d542d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid search on validation set\n",
    "f1_scorer = make_scorer(f1_score, pos_label=\"positive\")\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=1, scoring=f1_scorer)\n",
    "grid_search.fit(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd6a106",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding optimal n-gram range\n",
    "optimal_ngram = grid_search.best_params_['vectorizer__ngram_range']\n",
    "optimal_n_neighbors = grid_search.best_params_['classifier__n_neighbors']\n",
    "print(f\"Optimal n-gram: {optimal_ngram}\")\n",
    "print(f\"Optimal n_neighbors: {optimal_n_neighbors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978cbe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specific F1-score\n",
    "cv_results = grid_search.cv_results_\n",
    "f1_scores = cv_results['mean_test_score']\n",
    "param_combinations = cv_results['params']\n",
    "for params, f1 in zip(param_combinations, f1_scores):\n",
    "    print(f\"F1-score for {params}: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4042241c",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# KNN - BOW - Test set #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7617997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the best BOW KNN model - test data\n",
    "bow_vectorizer = CountVectorizer(ngram_range=(1, 1)) #Hier optimal n-gram range invullen\n",
    "X_train_vectorized = bow_vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = bow_vectorizer.transform(X_test)\n",
    "\n",
    "knn_model = KNeighborsClassifier(n_neighbors=9) #Hier optimal n_neighbors invullen\n",
    "knn_model.fit(X_train_vectorized, y_train)\n",
    "y_pred_test = knn_model.predict(X_test_vectorized)\n",
    "\n",
    "classification_rep = classification_report(y_test, y_pred_test)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129abd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# KNN - TFIDF - Hyperparameter tuning #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47475f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e7c004",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create pipeline TF-IDF\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', KNeighborsClassifier()) \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bd56a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter grid n-gram optimization\n",
    "param_grid = {\n",
    "    'vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)],\n",
    "    'classifier__n_neighbors': [1, 3, 5, 7, 9, 11, 13] \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddb1cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid search on validation set\n",
    "f1_scorer = make_scorer(f1_score, pos_label=\"positive\")\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=1, scoring=f1_scorer)\n",
    "grid_search.fit(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12df1d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding optimal n-gram range\n",
    "optimal_ngram = grid_search.best_params_['vectorizer__ngram_range']\n",
    "optimal_n_neighbors = grid_search.best_params_['classifier__n_neighbors']\n",
    "print(f\"Optimal n-gram: {optimal_ngram}\")\n",
    "print(f\"Optimal n_neighbors: {optimal_n_neighbors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15456aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specific F1-score\n",
    "cv_results = grid_search.cv_results_\n",
    "f1_scores = cv_results['mean_test_score']\n",
    "param_combinations = cv_results['params']\n",
    "for params, f1 in zip(param_combinations, f1_scores):\n",
    "    print(f\"F1-score for {params}: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4324677",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# KNN - TFIDF - Test set #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dada6974",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(2, 3)) #Hier optimal n-gram range invullen\n",
    "X_train_vectorized = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "knn_model = KNeighborsClassifier(n_neighbors=13) #Hier optimal n_neighbors invullen\n",
    "knn_model.fit(X_train_vectorized, y_train)\n",
    "y_pred_test = knn_model.predict(X_test_vectorized)\n",
    "\n",
    "classification_rep = classification_report(y_test, y_pred_test)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7c90ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# RF - BOW - Hyperparameter tuning #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e571be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5455944a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create pipeline \n",
    "pipeline_bow_rf = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', RandomForestClassifier()) \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfc1ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter grid n-gram optimization\n",
    "param_grid_rf = {\n",
    "    'vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)],\n",
    "    'classifier__n_estimators': [50, 100, 200,300], \n",
    "    'classifier__max_depth': [5, 10, 20,30], \n",
    "    'classifier__min_samples_split': [2, 5, 10, 15],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53326a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the model on training data\n",
    "pipeline_bow_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf539277",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid search on validation set\n",
    "f1_scorer = make_scorer(f1_score, pos_label=\"positive\")\n",
    "grid_search = GridSearchCV(pipeline_bow_rf, param_grid_rf, cv=5, n_jobs=1, scoring=f1_scorer)\n",
    "grid_search.fit(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662442f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding optimal n-gram range\n",
    "optimal_ngram = grid_search.best_params_['vectorizer__ngram_range']\n",
    "print(f\"Optimal n-gram: {optimal_ngram}\")\n",
    "\n",
    "optimal_params = grid_search.best_params_\n",
    "print(\"Optimal parameters:\")\n",
    "for param, value in optimal_params.items():\n",
    "    print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faaf00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specific F1-score (check)\n",
    "cv_results = grid_search.cv_results_\n",
    "f1_scores = cv_results['mean_test_score']\n",
    "param_combinations = cv_results['params']\n",
    "for params, f1 in zip(param_combinations, f1_scores):\n",
    "    print(f\"F1-score for {params}: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af86fb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# RF - BOW - Test set #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10b3b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizer = CountVectorizer(ngram_range=(1, 2)) #Hier optimal n-gram range invullen\n",
    "X_train_vectorized = bow_vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = bow_vectorizer.transform(X_test)\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=300, max_depth=30, min_samples_split=2) #Hier optimal values invullen\n",
    "rf_model.fit(X_train_vectorized, y_train)\n",
    "y_pred_test = rf_model.predict(X_test_vectorized)\n",
    "\n",
    "classification_rep = classification_report(y_test, y_pred_test)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9d3611",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# RF - TFIDF - Hyperparameter tuning #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8274b1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af86f0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create pipeline TF-IDF\n",
    "pipeline_tfidf_rf = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', RandomForestClassifier()) \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7c26dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter grid n-gram optimization\n",
    "param_grid_rf = {\n",
    "    'vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)],\n",
    "    'classifier__n_estimators': [50, 100, 200,300], \n",
    "    'classifier__max_depth': [5, 10, 20,30], \n",
    "    'classifier__min_samples_split': [2, 5, 10, 15],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4f0387",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the model on training data\n",
    "pipeline_tfidf_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cb1bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid search on validation set\n",
    "f1_scorer = make_scorer(f1_score, pos_label=\"positive\")\n",
    "grid_search = GridSearchCV(pipeline_tfidf_rf, param_grid_rf, cv=5, n_jobs=1, scoring=f1_scorer)\n",
    "grid_search.fit(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace31e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding optimal n-gram range\n",
    "optimal_ngram = grid_search.best_params_['vectorizer__ngram_range']\n",
    "print(f\"Optimal n-gram: {optimal_ngram}\")\n",
    "\n",
    "optimal_params = grid_search.best_params_\n",
    "print(\"Optimal parameters:\")\n",
    "for param, value in optimal_params.items():\n",
    "    print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4783e7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specific F1-score (check)\n",
    "cv_results = grid_search.cv_results_\n",
    "f1_scores = cv_results['mean_test_score']\n",
    "param_combinations = cv_results['params']\n",
    "for params, f1 in zip(param_combinations, f1_scores):\n",
    "    print(f\"F1-score for {params}: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fd4c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# RF - TFIDF - Test set #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f159d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2)) #Hier optimal n-gram range invullen\n",
    "X_train_vectorized = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=300, max_depth=30, min_samples_split=2) #Hier optimal values invullen\n",
    "rf_model.fit(X_train_vectorized, y_train)\n",
    "y_pred_test = rf_model.predict(X_test_vectorized)\n",
    "\n",
    "classification_rep = classification_report(y_test, y_pred_test)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dfd2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# RNN - BOW - Hyperparameter tuning #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda9d6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08d189a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "# Fit and transform the documents\n",
    "count_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get the feature names (terms)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Construct the sequential Count vectors\n",
    "sequential_count_vectors = []\n",
    "for i in range(len(documents)):\n",
    "    document_vector = count_matrix[i].toarray().flatten()\n",
    "    sequential_count_vector = [document_vector[feature_names.index(term)] for term in documents[i].split()]\n",
    "    sequential_count_vectors.append(sequential_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fc4315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit countvectorizer on text data\n",
    "count_vectorizer = CountVectorizer(max_features=5000, ngram_range=(1, 2)) #Aanpassen hyperparameter tuning\n",
    "X_train_count = count_vectorizer.fit_transform(X_train)\n",
    "X_val_count = count_vectorizer.transform(X_val)\n",
    "X_test_count = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e188a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd67548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text data to sequences\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeeb5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to a fixed length\n",
    "maxlen = 100\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=maxlen)\n",
    "X_val_padded = pad_sequences(X_val_seq, maxlen=maxlen)\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddaf16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string labels to numerical labels\n",
    "y_train_numeric = y_train.map({'positive': 1, 'negative': 0})\n",
    "y_val_numeric = y_val.map({'positive': 1, 'negative': 0})\n",
    "y_test_numeric = y_test.map({'positive': 1, 'negative': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a94e5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Keras model function\n",
    "def create_model(units=50, activation='relu', output_dim=50, learning_rate=0.001, dropout_rate=0.2):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=output_dim, input_length=maxlen))\n",
    "    model.add(SimpleRNN(units=units, activation=activation))\n",
    "    model.add(Dropout(rate=dropout_rate)) \n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b47ac77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the KerasClassifier for GridSearchCV\n",
    "model = KerasClassifier(build_fn=create_model, epochs=5, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbb3844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters and their ranges\n",
    "param_grid = {\n",
    "    'units': [50, 100, 150],\n",
    "    'output_dim': [50, 100, 200],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'dropout_rate': [0.2, 0.5, 0.7], \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447ebda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform GridSearchCV\n",
    "f1_scorer = make_scorer(f1_score, pos_label=1)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=f1_scorer, cv=2)\n",
    "grid_result = grid.fit(X_train_padded, y_train_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc84e9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check best parameters\n",
    "Best params\n",
    "print(\"Best Parameters based on F1 score: \", grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fe4173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific F1-score (check)\n",
    "cv_results = grid_result.cv_results_\n",
    "f1_scores = cv_results['mean_test_score']\n",
    "param_combinations = cv_results['params']\n",
    "for params, f1 in zip(param_combinations, f1_scores):\n",
    "    print(f\"F1-score for {params}: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1629a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# RNN - BOW - Test #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859841dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d01fd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input paramaters to train the model\n",
    "manual_params = {\n",
    "    'units': 150,\n",
    "    'output_dim': 200,\n",
    "    'learning_rate': 0.001,\n",
    "    'dropout_rate': 0.5\n",
    "}\n",
    "\n",
    "model_test = create_model(units=manual_params['units'],\n",
    "                             output_dim=manual_params['output_dim'],\n",
    "                             learning_rate=manual_params['learning_rate'],\n",
    "                             dropout_rate=manual_params['dropout_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8e43a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "model_test.fit(X_train_padded, y_train_numeric, epochs=5, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a35da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "y_pred_manual = model_test.predict(X_test_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a95e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predicted probabilities to binary labels\n",
    "y_pred_binary_manual = (y_pred_manual > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73a67a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_numeric, y_pred_binary_manual))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_numeric, y_pred_binary_manual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f14e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# RNN - TFIDF - Hyperparameter tuning #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431bf73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0927f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature names (terms)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Construct the sequential TF-IDF vectors\n",
    "sequential_tfidf_vectors = []\n",
    "for i in range(len(documents)):\n",
    "    document_vector = tfidf_matrix[i].toarray().flatten()\n",
    "    sequential_tfidf_vector = [document_vector[feature_names.index(term)] for term in documents[i].split()]\n",
    "    sequential_tfidf_vectors.append(sequential_tfidf_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0b0ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2)) #aanpassen\n",
    "\n",
    "# Fit and transform the documents\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get the feature names (terms)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Construct the sequential TF-IDF vectors\n",
    "sequential_tfidf_vectors = []\n",
    "for i in range(len(documents)):\n",
    "    document_vector = tfidf_matrix[i].toarray().flatten()\n",
    "    sequential_tfidf_vector = [document_vector[feature_names.index(term)] for term in documents[i].split()]\n",
    "    sequential_tfidf_vectors.append(sequential_tfidf_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c30a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit TF-IDF vectorizer on text data\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2)) #n-gram aanpassen\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06935163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train) #aanpassen naar vectorization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0c4d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text data to sequences\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee06a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to a fixed length\n",
    "maxlen = 100\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=maxlen)\n",
    "X_val_padded = pad_sequences(X_val_seq, maxlen=maxlen)\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a648a77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string labels to numerical labels\n",
    "y_train_numeric = y_train.map({'positive': 1, 'negative': 0})\n",
    "y_val_numeric = y_val.map({'positive': 1, 'negative': 0})\n",
    "y_test_numeric = y_test.map({'positive': 1, 'negative': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bac56a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Keras model function\n",
    "def create_model(units=50, activation='relu', output_dim=50, learning_rate=0.001, dropout_rate=0.2):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=output_dim, input_length=maxlen))\n",
    "    model.add(SimpleRNN(units=units, activation=activation))\n",
    "    model.add(Dropout(rate=dropout_rate)) \n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4051ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the KerasClassifier for GridSearchCV\n",
    "model = KerasClassifier(build_fn=create_model, epochs=5, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcd6fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters and their ranges\n",
    "param_grid = {\n",
    "    'units': [50, 100, 150],\n",
    "    'output_dim': [50, 100, 200],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'dropout_rate': [0.2, 0.5, 0.7], \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6f89ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform GridSearchCV\n",
    "f1_scorer = make_scorer(f1_score, pos_label=1)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=f1_scorer, cv=2)\n",
    "grid_result = grid.fit(X_train_padded, y_train_numeric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5d62e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check best parameters\n",
    "print(\"Best Parameters based on F1 score: \", grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a3117e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific F1-score (check)\n",
    "cv_results = grid_result.cv_results_\n",
    "f1_scores = cv_results['mean_test_score']\n",
    "param_combinations = cv_results['params']\n",
    "for params, f1 in zip(param_combinations, f1_scores):\n",
    "    print(f\"F1-score for {params}: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3804cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# RNN - TFIDF - Test #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e59bd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aac4430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input paramaters to train the model\n",
    "manual_params = {\n",
    "    'units': 150,\n",
    "    'output_dim': 200,\n",
    "    'learning_rate': 0.001,\n",
    "    'dropout_rate': 0.5\n",
    "}\n",
    "\n",
    "model_test = create_model(units=manual_params['units'],\n",
    "                             output_dim=manual_params['output_dim'],\n",
    "                             learning_rate=manual_params['learning_rate'],\n",
    "                             dropout_rate=manual_params['dropout_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09f16f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "model_test.fit(X_train_padded, y_train_numeric, epochs=5, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab33ddc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "y_pred_manual = model_test.predict(X_test_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01288e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predicted probabilities to binary labels\n",
    "y_pred_binary_manual = (y_pred_manual > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229e8df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_numeric, y_pred_binary_manual))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_numeric, y_pred_binary_manual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025dc344",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
